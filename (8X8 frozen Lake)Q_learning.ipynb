{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.1 - Q-learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LxioR8F-hhD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5T021tK864t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51ddcf2-1cd5-493e-a153-040b213445a6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "print(tf.__version__)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya1h3DnZAWt4"
      },
      "source": [
        "# Reinforcement learning\n",
        "\n",
        "Reinforcement learning (RL) is an area of ML related to training an intelligent agent which performs a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. Via trials and errors the agent comes up with a solution to the problem. The agent gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\n",
        "\n",
        "Ref: https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2b6ZMcZQGyB"
      },
      "source": [
        "## Frozen lake environment\n",
        "\n",
        "Let's consider simple environment and agent acting in it. `gym` toolkit provides collection of environments that can be simulated in RL suitable way. \n",
        "\n",
        "(F)rozen lake is field with size 4x4 units. There are (H)oles in the ice scattered around. (S)tart and (G)oal positions are defined.\n",
        "\n",
        "The grand objective is to train the agent (marked in red) in such a way that it can successfully cross the lake.\n",
        "\n",
        "Agent gets reward of 1 if it reaches the goal, and 0 otherwise.\n",
        "\n",
        "\n",
        "> Note: `is_slippery=False` disables randomness in action results (for demo purposes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz2ziiIOPrjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec44115-b214-466e-8afd-c71241769d21"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "observation = env.reset()\n",
        "env.render()\n",
        "\n",
        "print()\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Action space:', env.action_space)\n",
        "print('Current observation:', observation)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "Observation space: Discrete(16)\n",
            "Action space: Discrete(4)\n",
            "Current observation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw-1DOMZaNrD"
      },
      "source": [
        "Let's implement simple agent, which randomly wanders around the lake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqJeNOcMZh10"
      },
      "source": [
        "env.reset()\n",
        "for step in range(10):\n",
        "  # randomly select next action\n",
        "  action = env.action_space.sample()\n",
        "\n",
        "  # agent performs the selected action, and gets feedback from the environment\n",
        "  state, reward, done, info = env.step(action)  \n",
        "\n",
        "  print('===============================================')\n",
        "  print(f'Step: {step}\\tAction: {action}\\tReward: {reward}\\tNew state: {state}\\n')\n",
        "  env.render()\n",
        "  print()\n",
        "\n",
        "  if done:\n",
        "    print('\\nEpisode ended')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92hRPkeLeXz8"
      },
      "source": [
        "The essence of RL is to run number of episodes and modify policy in such a way to get maximum cumulative reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxxYN5-chBb0"
      },
      "source": [
        "# Q-learning\n",
        "\n",
        "Q-learning is one of RL algorithms, it finds optimal policy and is well suited for discrete problems.\n",
        "\n",
        "\"Q\" refers to the function that the algorithm computes: the expected (future) rewards for an action taken in a given state.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Q-learning\n",
        "\n",
        "Usually the algorithm is implemneted as table of Q values for each state-action. Using the table current state can be mapped to the best next action.\n",
        "\n",
        "|         | State A | State B | State C |\n",
        "|---------|--------:|--------:|--------:|\n",
        "|Action 1 | 0.17    | 0.56    | 12.43   |\n",
        "|Action 2 | 0.22    | 0.32    | 32.10   |\n",
        "\n",
        "The essense of the algorithm lies in how the values of the table are changed.\n",
        "\n",
        "$$\n",
        "Q^{new}(s_t, a_t) \\leftarrow \n",
        "\\underbrace{ Q(s_t, a_t) }_\\textrm{old value}\n",
        " + \\underbrace{a}_\\textrm{learning rate}\n",
        "\\cdot \\left(\n",
        "   \\underbrace{r_t}_\\textrm{reward} \n",
        "   + \\underbrace{\\gamma}_\\textrm{discount factor}\n",
        "   \\cdot \\underbrace{\\max_a{Q(s_{t+1}, a)}}_\\textrm{best future value}\n",
        "   - \\underbrace{ Q(s_t, a_t) }_\\textrm{old value} \n",
        "\\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "* $a$ - determiates how fast new values are applied to the table:\n",
        "  * $0$ - values never change;\n",
        "  * $1$ - old values are overwritten by new values;\n",
        "* $\\gamma$ - determinates how much importance has the expected future reward:\n",
        "  * $0$ - only currect step reward is considered;\n",
        "  * $1$ - infinitely long future is expected (not realistic);\n",
        "\n",
        "\n",
        "Perhaps more readable / understandable formula looks like follows:\n",
        "$$\n",
        "Q^{new}(s_t, a_t) \\leftarrow \n",
        "\\left(1 - a \\right) \\cdot \\underbrace{ Q(s_t, a_t) }_\\textrm{old value}\n",
        " + a \\cdot \n",
        " \\left(\n",
        "   \\underbrace{r_t}_\\textrm{reward} \n",
        "   + \\underbrace{\\gamma}_\\textrm{discount factor}\n",
        "   \\cdot \\underbrace{\\max_a{Q(s_{t+1}, a)}}_\\textrm{best future value}   \n",
        "\\right)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbipAPGfurAD"
      },
      "source": [
        "Q = np.zeros(shape=(env.observation_space.n, env.action_space.n))\n",
        "# untrained Q-table\n",
        "Q "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwTG2uPuuql7"
      },
      "source": [
        "In order to facilitate better and faster training, few tricks are usually applied.\n",
        "\n",
        "## Exploration vs Exploitation\n",
        "If only the actions with the highest score are selected from the table, the agent very quickly starts to focus on previous succesfull steps (**exploitation**).\n",
        "\n",
        "While for training purposes and good global solution the **exploration** is needed (let the agent make mistakes and perhaps it will find better solution).\n",
        "\n",
        "Usually the ratio between exploration and exploitation is parametrized and decays over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J9q3TguEJnA"
      },
      "source": [
        "## Continous reward\n",
        "\n",
        "Straightforward reward mechanism can be implemented as the reward only for achieving the goal (like in Frozen lake environment, where agent gets reward of $1$ for reaching the goal position, and $0$ otherwise).\n",
        "\n",
        "However usually it is better to provide at least some feedback (reward) for each step taked by agent, even if the reward is negative. It can be interpreted as resources or time spent for the step. This approach prevents agents from looping around infinetly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv7KnEMpJpGh"
      },
      "source": [
        "# Code example\n",
        "\n",
        "First, we need a few utility functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxWsNqat-rZg"
      },
      "source": [
        "def select_action(observation, explore = 0.0):\n",
        "  if np.random.uniform() < explore:\n",
        "    # exploring the environment, taking random action\n",
        "    return env.action_space.sample()\n",
        "  else:\n",
        "    # finding best action for current observation\n",
        "    return np.argmax(Q[observation, :])\n",
        "\n",
        "def update_table(observation, observation_next, action, reward):\n",
        "  learning_rate =  0.7\n",
        "  future_factor = 0.9\n",
        "  old_value = Q[observation, action]\n",
        "  new_value = reward + future_factor * np.max(Q[observation_next, :])\n",
        "  # updating table value according to formula\n",
        "  Q[observation, action] = old_value + learning_rate * (new_value - old_value)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX17Ny21IEVe"
      },
      "source": [
        "# single step demonstration\n",
        "state = env.reset()\n",
        "action = select_action(state, explore=1.0)\n",
        "new_state, reward, done, _ = env.step(action)\n",
        "reward -= 0.01 # adding small negative reward for perfroming the step (spent time, energy)\n",
        "\n",
        "update_table(state, new_state, action, reward)\n",
        "\n",
        "# output of the table\n",
        "env.render()\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX822XwEPQYT"
      },
      "source": [
        "Training the model implies running number of episodes and let the agent explore the environment and find best policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1q4UOORGuTB"
      },
      "source": [
        "explore = 0.9 # reset exploration rate\n",
        "\n",
        "# how many training episodes we want to run\n",
        "for episode in range(10000):\n",
        "  state = env.reset()  \n",
        "\n",
        "  # how many steps our agent tries to perform in each episode\n",
        "  for step in range(100):\n",
        "    action = select_action(state, explore)\n",
        "    new_state, reward, done, _ = env.step(action)\n",
        "    reward -= 0.01\n",
        "    \n",
        "    update_table(state, new_state, action, reward)\n",
        "    state = new_state\n",
        "\n",
        "    if done:\n",
        "      # episode ended, either win or lose\n",
        "      break\n",
        "  \n",
        "  # after each episode\n",
        "  explore = explore * 0.999 # decay exploration factor\n",
        "  explore = max(explore, 0.05) # usually some exploration probability is kept during the training\n",
        "\n",
        "# trained Q-table\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRoS4r6xP7n2"
      },
      "source": [
        "Now when the agent is trained, it can exploit its knowledge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOOiOJ7OJLnI"
      },
      "source": [
        "state = env.reset()\n",
        "\n",
        "for step in range(15):\n",
        "  action = select_action(state)\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  env.render()\n",
        "  print()\n",
        "  if done:\n",
        "    if reward > 0:\n",
        "      print(\"Win!\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmHqBMvuVitC"
      },
      "source": [
        "# Task - Train Q-learning model for 8x8 Frozen lake environment\n",
        "\n",
        "By default, Frozen lake environment includes various map configurations and *is slippery* (there is random chance that agent is not successful in action).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCflGzwrYS5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d388efa-2128-43e5-f767-367c5318a37b"
      },
      "source": [
        "env = gym.make('FrozenLake-v0', map_name=\"8x8\")\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIDL53XV3hSv",
        "outputId": "d7c77ade-faeb-4feb-9a8d-cd298ee3cd7d"
      },
      "source": [
        "# TODO: build Q model\n",
        "#Q-model\n",
        "action_size = env.action_space.n\n",
        "print(\"Action size: \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size: \", state_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action size:  4\n",
            "State size:  64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvFTeMCF3jiC",
        "outputId": "c18c6832-a26f-4a77-d638-996279956c45"
      },
      "source": [
        "#Q-table\n",
        "q_table = np.zeros((state_size, action_size))\n",
        "print(q_table)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lTEGPo14Gn1"
      },
      "source": [
        "num_episodes = 10000       # Total episodes\n",
        "learning_rate = 0.3          # Learning rate\n",
        "max_steps = 200               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "exploration_rate = 1.0                 # Exploration rate\n",
        "max_exploration_rate = 1.0             # Exploration probability at start\n",
        "min_exploration_rate = 0.01            # Minimum exploration probability \n",
        "exploration_decay_rate = 0.002            # Exponential decay rate for exploration prob"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG2VWUzx4yGh",
        "outputId": "bb2f0225-131d-4395-d8d0-8e57ed912fd9"
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episodes in range(num_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_rate_threshold = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than exploration_rate --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_rate_threshold > exploration_rate:\n",
        "            action = np.argmax(q_table[state,:])\n",
        "            #print(exp_rate_threshold, \"action\", action)\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "            #print(\"action random\", action)\n",
        "            \n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce exploration_rate (because we need less and less exploration)\n",
        "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate)*np.exp(-exploration_decay_rate*episodes) \n",
        "    rewards.append(total_rewards)\n",
        "    \n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/num_episodes))\n",
        "print(q_table)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 0.3777\n",
            "[[3.13888009e-02 3.13342964e-02 3.97625670e-02 3.13316105e-02]\n",
            " [3.43979295e-02 3.32496903e-02 3.46009885e-02 5.04662746e-02]\n",
            " [3.90879787e-02 3.92230429e-02 5.20078302e-02 4.25245260e-02]\n",
            " [5.64576131e-02 5.62062835e-02 6.69810379e-02 5.60162154e-02]\n",
            " [5.99709881e-02 6.50878780e-02 8.41874461e-02 6.12863304e-02]\n",
            " [7.59583477e-02 7.33394235e-02 9.95360388e-02 7.60263536e-02]\n",
            " [8.71820322e-02 9.12080887e-02 1.05156196e-01 8.24192232e-02]\n",
            " [7.86519158e-02 9.81837819e-02 1.08784135e-01 8.94387783e-02]\n",
            " [3.20145564e-02 3.04639041e-02 3.22969246e-02 3.82058081e-02]\n",
            " [2.98287467e-02 3.32869736e-02 3.33637054e-02 4.48872024e-02]\n",
            " [3.33619314e-02 3.65898012e-02 2.97581473e-02 5.53533085e-02]\n",
            " [3.04613150e-02 4.79579318e-02 4.97820969e-02 7.06455237e-02]\n",
            " [6.52843508e-02 5.25786623e-02 9.36294097e-02 6.47458932e-02]\n",
            " [7.27922656e-02 8.62500955e-02 1.13529303e-01 7.64682862e-02]\n",
            " [8.74021518e-02 8.75386213e-02 1.23866772e-01 8.78420893e-02]\n",
            " [9.89187988e-02 9.55168667e-02 1.26176402e-01 1.00531060e-01]\n",
            " [1.72777040e-02 2.14523956e-02 2.63295044e-02 3.12627334e-02]\n",
            " [2.49947225e-02 1.63596357e-02 2.97919129e-02 3.36689300e-02]\n",
            " [3.28571782e-02 7.09075620e-03 4.02429100e-03 1.37153093e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.88578888e-02 2.01791796e-02 9.21585162e-02 1.73053576e-02]\n",
            " [3.96921048e-02 8.41319420e-02 4.64638171e-02 1.23662280e-01]\n",
            " [9.60539680e-02 9.88027113e-02 1.57743412e-01 9.36685163e-02]\n",
            " [1.31398737e-01 1.48799895e-01 1.27620305e-01 1.17161051e-01]\n",
            " [1.35302559e-02 1.42237257e-02 1.42657993e-02 2.55701354e-02]\n",
            " [2.48520756e-02 1.12906246e-02 1.38546538e-02 1.19532182e-02]\n",
            " [9.97344216e-03 8.53060466e-03 1.19265803e-02 2.55090317e-02]\n",
            " [9.36684673e-04 8.96459463e-04 1.15522966e-03 4.08322432e-02]\n",
            " [6.46854103e-02 1.16243538e-02 2.72519328e-02 1.39974189e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.12622660e-01 7.80436334e-02 1.68057989e-01 7.02406481e-02]\n",
            " [1.53105988e-01 1.75371161e-01 1.38444810e-01 1.51416774e-01]\n",
            " [2.23526393e-02 1.04222565e-02 1.00686131e-02 1.04132164e-02]\n",
            " [4.30576501e-03 6.29928429e-03 6.77223217e-03 1.62755414e-02]\n",
            " [1.89364832e-03 1.81555041e-03 1.56041397e-03 4.99683800e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.62886429e-02 3.80700573e-02 7.15907407e-02 1.75297499e-02]\n",
            " [3.19808814e-02 1.12904764e-01 5.00470154e-02 3.77110328e-02]\n",
            " [3.93842617e-02 5.52775502e-02 1.48111940e-01 1.38194106e-01]\n",
            " [1.82515658e-01 2.16286055e-01 2.62189727e-01 1.93439555e-01]\n",
            " [1.36160702e-02 8.41792052e-03 8.39589346e-03 6.79991065e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [4.14453903e-03 2.18698551e-02 2.74348480e-03 1.52445364e-05]\n",
            " [5.69950588e-03 1.30881569e-02 6.85527199e-03 7.29409439e-02]\n",
            " [1.00774552e-01 3.38853763e-02 2.77434445e-02 3.94945957e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.01652167e-01 2.12320076e-01 4.80707635e-01 2.53515039e-01]\n",
            " [8.22077833e-03 4.63186649e-03 2.86278543e-03 4.46165003e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.28457003e-04 2.72157226e-03 9.04634956e-04 4.31203135e-04]\n",
            " [6.53823841e-03 5.17918080e-04 1.12430338e-04 4.21417162e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.63378006e-02 1.27414865e-02 1.60071610e-01 1.64701215e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [2.70920595e-01 3.66454607e-01 7.92164452e-01 4.92911047e-01]\n",
            " [5.45169523e-03 4.48631192e-03 4.67089388e-03 4.68627584e-03]\n",
            " [2.07753513e-03 4.00668079e-03 2.36670714e-03 2.17008915e-03]\n",
            " [1.77799705e-03 1.60600237e-03 1.51062910e-03 1.67463266e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.38763529e-03 5.77356445e-02 4.58340414e-02 1.81346581e-02]\n",
            " [1.70229055e-01 2.98472016e-01 4.49546725e-01 2.17648505e-01]\n",
            " [2.45270320e-01 7.17673608e-01 3.25361710e-01 3.01526564e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s616mZAUQC8A",
        "outputId": "a157624a-31cb-40b0-adec-1830a8964af2"
      },
      "source": [
        "# watching the agent play\n",
        "\n",
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode+1)\n",
        "    time.sleep(1)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "         clear_output(wait=True)\n",
        "         env.render()\n",
        "         time.sleep(0.3)\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "         action = np.argmax(q_table[state,:])\n",
        "         new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "         if done:\n",
        "            clear_output(wait=True)\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            if new_state == 63:\n",
        "                print(\"We reached the Goal\")\n",
        "                time.sleep(0.3)\n",
        "            else:  \n",
        "                print(\"We fell in the a hole \")\n",
        "                time.sleep(0.3)            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            \n",
        "            break\n",
        "            state = new_state\n",
        "env.close()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "We fell in the a hole \n",
            "Number of steps 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPZ4xzYXZYtI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
